defaults:
  - _self_
  - override hydra/job_logging: colorlog
  - override hydra/hydra_logging: colorlog

hydra:
  job:
    env_set:
      DGLBACKEND: tensorflow
      TF_CPP_MIN_LOG_LEVEL: 1
      # TF_NUM_INTEROP_THREADS: 0
      # TF_NUM_INTRAOP_THREADS: 0
      VERBOSE: 1
  verbose: false

# Python default: 3000
recursionlimit: null

szs_status_of_interest:
  # https://www.tptp.org/cgi-bin/SeeTPTP?Category=Documents&File=SZSOntology
  # Unsatisfiable
  - THM
  - CAX
  - UNS
  # ResourceOut
  - RSO
  - TMO
  - MMO
  - INO
  - ACO
  # GaveUp
  - GUP
  - INC

seed: 0
epochs: null
train_ratio: 0.5
max_problem_count: null
eval:
  # Evaluate a baseline
  baseline: true
  # Evaluate the model before any training (after random initialization)
  initial: true
proof_sample_weight: 0.5
baseline_files: {}
evaluation:
  empirical:
    start: -1
    step: 1000
  proxy:
    start: -1
    step: 100
per_problem_stats: false
join_searches: false
loop_iterations: 10


initial_design:
  distribution:
    # Choices: lognorm, norm, expon, uniform
    # https://docs.scipy.org/doc/scipy/reference/stats.html
    name: uniform
    loc: 1
    scale: 0

# Maximum proof output file size in bytes
max_proof_file_size: null

limits:
  train:
    # Maximum number of proof searches in a batch.
    max_batch_samples: 32
    # Maximum size of the tensors that represent proof searches in one batch.
    max_batch_size: null
    # Maximum size of the tensor that represents one training proof search.
    # The size of the tensor is `proof_clauses * nonproof_clauses * symbols`.
    # Proof searches are subsampled to meet this limit.
    max_sample_size: 100000
  predict:
    max_batch_samples: 64
    max_batch_size: 1000000
    max_sample_size: 100000

# Choices: literal_positive, literal_negative, equality, inequality, variable_occurrence, variable_count, number
clause_features:
  - variable_occurrence
  - equality

vampire_cmd: vampire
problem:
  lists: []
  names: []
tptp_path: ${oc.env:TPTP}

evaluation_problems:
  train: null
  val: null

options:
  common:
    include: ${tptp_path}
    input_syntax: tptp
    avatar: "off"
    # The default saturation algorithm LRS does not currently support custom functor weights.
    saturation_algorithm: discount
    age_weight_ratio: "1:5"
    statistics: full
    time_limit: 300
  probe:
    proof: "off"
    instruction_limit: 50000
  verbose:
    #show_everything: "on"
    show_active: "on"
    proof_extra: free
    show_clause_features: "on"
  evaluation:
    default: {}
    #awr_0_1:
    #  age_weight_ratio: "0:1"
    #awr_1_9:
    #  age_weight_ratio: "1:9"
  clausify: {}

clausify_timeout: 310

probe_run_args:
  # Choices: subprocess, benchexec
  backend: subprocess

  # subprocess: The parameters are passed to `subprocess.run`.
  timeout: 310

  # BenchExec: The arguments are passed to `benchexec.runexecutor.RunExecutor.execute_run`.
  # Notable parameters: softtimelimit, hardtimelimit, walltimelimit
  # Documentation: https://github.com/sosy-lab/benchexec/blob/main/doc/runexec.md

batch:
  size: 64
  count: 1

parallel:
  # Choices: loky, threading
  backend: loky
  n_jobs: 1

gcn:
  depth: 4
  message_size: 16
  activation: relu
  # Options: concat, sum
  aggregate: sum
  dropout:
    input: null
    hidden: null
  max_norm: null
  residual: true
  layer_norm: true
  # If 'custom', use a recommended normalization for each edge type.
  # Choices: both, right, none, custom
  conv_norm: both
  max_problem_nodes:
    train: 100000
    val: 100000

max_problem_nodes: 10000

embedding_to_cost:
  # Transforms an embedding of a clause component (symbol, variables, equalities)
  # to the weight of a corresponding clause feature (symbol occurrence count, variable occurrence count,
  # equality occurrence count).
  # TODO: Support deeper MLPs.
  activation: softplus
  output_bias: 1
  regularization:
    l1: 0.0
    l2: 0.0

symbol_cost:
  # Factor of L2 regularization penalty on symbol cost values
  l2: 0.0

# Choices: adam, sgd, rmsprop
optimizer: adam
learning_rate: 0.001

tf:
  device: null
  run_eagerly: false
  log_device_placement: false

tensorboard:
  # https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard
  log_dir: logs
  histogram_freq: 1
  profile_batch: 0
  embeddings_freq: 1

reduce_lr_on_plateau:
  # https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ReduceLROnPlateau
  monitor: loss
  factor: 0.5
  patience: 10
  verbose: 1

early_stopping:
  # https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping
  monitor: val_binary_accuracy
  patience: 100
  baseline: 0.5
