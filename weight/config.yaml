defaults:
  - _self_
  - override hydra/job_logging: colorlog
  - override hydra/hydra_logging: colorlog

hydra:
  job:
    env_set:
      DGLBACKEND: tensorflow
      TF_CPP_MIN_LOG_LEVEL: 1
      # Python default: 1000
      RECURSIONLIMIT: 10000
      # TF_NUM_INTEROP_THREADS: 0
      # TF_NUM_INTRAOP_THREADS: 0

seed: 0
epochs: null
train_ratio: 0.5
max_problem_count: null
eval:
  baseline: true
  initial: true
proof_sample_weight: 0.5
baseline_files: {}
empirical:
  start: -1
  step: 1
per_problem_stats: false

proof:
  # Maximum proof output file size in bytes
  max_size: 100000000
  max_clauses:
    proof: 1000
    nonproof: 1000
  max_symbols: 1000

# Choices: literal_positive, literal_negative, equality, inequality, variable_occurrence, variable_count, number
clause_features:
  - variable_occurrence
  - equality

workspace_dir: null
vampire_cmd: vampire
problem:
  list_file: null
  names: []
tptp_path: null

evaluation_problems:
  train: null
  val: null

options:
  common:
    include: ${tptp_path}
    input_syntax: tptp
    avatar: "off"
    # The default saturation algorithm LRS does not currently support custom functor weights.
    saturation_algorithm: discount
    age_weight_ratio: "1:5"
  probe:
    proof: "off"
    instruction_limit: 50000
  verbose:
    #show_everything: "on"
    show_active: "on"
    proof_extra: free
  evaluation:
    default: {}
    #awr_0_1:
    #  age_weight_ratio: "0:1"
    #awr_1_9:
    #  age_weight_ratio: "1:9"

probe_run_args:
  # Choices: subprocess, benchexec
  backend: subprocess

  # subprocess: The parameters are passed to `subprocess.run`.
  timeout: 60

  # BenchExec: The arguments are passed to `benchexec.runexecutor.RunExecutor.execute_run`.
  # Notable parameters: softtimelimit, hardtimelimit, walltimelimit
  # Documentation: https://github.com/sosy-lab/benchexec/blob/main/doc/runexec.md

batch:
  size: 64
  count: 1

parallel:
  # Choices: loky, threading
  backend: loky
  n_jobs: 1
  verbose: 1

gcn:
  depth: 4
  message_size: 16
  activation: relu
  # Options: concat, sum
  aggregate: sum
  dropout:
    input: null
    hidden: null
  max_norm: null
  residual: true
  layer_norm: true
  # If 'custom', use a recommended normalization for each edge type.
  # Choices: both, right, none, custom
  conv_norm: both
  max_problem_nodes:
    train: 100000
    val: 100000

max_problem_nodes: 10000

embedding_to_cost:
  # TODO: Support deeper MLPs.
  activation: softplus
  output_bias: 1
  regularization:
    l1: 0.0
    l2: 0.0

symbol_cost:
  # Factor of L2 regularization penalty on symbol cost values
  l2: 0.0

# Choices: adam, sgd, rmsprop
optimizer: adam
learning_rate: 0.001

tf:
  device: null
  run_eagerly: false
  log_device_placement: false

tensorboard:
  # https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard
  log_dir: logs
  histogram_freq: 1
  profile_batch: 0
  embeddings_freq: 1

reduce_lr_on_plateau:
  # https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ReduceLROnPlateau
  monitor: loss
  factor: 0.5
  patience: 10
  verbose: 1

early_stopping:
  # https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping
  monitor: val_binary_accuracy
  patience: 100
  baseline: 0.5
